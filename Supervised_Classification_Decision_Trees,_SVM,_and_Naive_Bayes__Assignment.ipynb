{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer.  Information Gain is a concept used in Decision Trees to select the best feature for splitting the data at each node.\n",
        "\n",
        "-  measures how much the uncertainty or impurity (called entropy) of the dataset is reduced when we split the data based on a particular feature. Before splitting, the data has some randomness. After splitting on a good feature, the data becomes more organized and pure. The amount of this improvement is called Information Gain.\n",
        "\n",
        "- In a Decision Tree, Information Gain is calculated for all features, and the feature with the highest Information Gain is chosen for the split. This process is repeated at each level of the tree until the data is classified or a stopping condition is reached.\n",
        "\n",
        "In simple words:\n",
        "*italicised text*\n",
        "Information Gain helps the decision tree decide which feature gives the most useful information for making correct decisions."
      ],
      "metadata": {
        "id": "9qd2y_pxz80W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 2 : What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        " Answer. Gini Impurity and Entropy are both measures used in Decision Trees to evaluate how ‚Äúpure‚Äù or ‚Äúimpure‚Äù a dataset is, but they differ in calculation and behavior.\n",
        "\n",
        "1. **Gini Impurity**\n",
        "\n",
        "Measures the probability that a randomly chosen data point would be incorrectly classified\n",
        "\n",
        "Formula:\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí‚àëp\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Value range:\n",
        "\n",
        "0 ‚Üí pure node (only one class)\n",
        "\n",
        "Higher value ‚Üí more impurity\n",
        "\n",
        "Used in: CART (Classification and Regression Trees)\n",
        "\n",
        "Faster to compute, so commonly used in practice\n",
        "\n",
        "2. **Entropy**\n",
        "\n",
        "Measures the amount of uncertainty or randomness in the data\n",
        "\n",
        "Formula:\n",
        "\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí‚àëp\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Value range:\n",
        "\n",
        "0 ‚Üí pure node\n",
        "\n",
        "1 (for binary classification) ‚Üí maximum impurity\n",
        "\n",
        "Used in: ID3 and C4.5 algorithms\n",
        "\n",
        "More mathematically complex due to logarithms\n",
        "\n",
        "3. **Key Differences**\n",
        "| Aspect      | Gini Impurity                   | Entropy                              |\n",
        "| ----------- | ------------------------------- | ------------------------------------ |\n",
        "| Meaning     | Misclassification probability   | Degree of randomness                 |\n",
        "| Formula     | (1 - \\sum p_i^2)                | (-\\sum p_i \\log_2 p_i)               |\n",
        "| Computation | Simpler, faster                 | Slower (uses log)                    |\n",
        "| Used in     | CART                            | ID3, C4.5                            |\n",
        "| Sensitivity | Less sensitive to class changes | More sensitive to class distribution |\n",
        "\n",
        "4. **Summary**\n",
        "\n",
        "- Both aim to find the best split in a decision tree\n",
        "\n",
        "- Gini is computationally efficient\n",
        "\n",
        "- Entropy gives a more theoretical measure of uncertainty\n",
        "\n",
        "- In practice, both often produce similar trees"
      ],
      "metadata": {
        "id": "eksBdQ101fyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 : What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer. **Pre-Pruning in Decision Trees** is a technique used to **stop the growth of the tree early** in order to avoid overfitting.\n",
        "\n",
        "Instead of allowing the decision tree to grow completely, **pre-pruning sets rules in advance** to decide when to stop splitting a node. If a split does not significantly improve the model, it is not performed.\n",
        "\n",
        "### Common Pre-Pruning Criteria:\n",
        "\n",
        "* Maximum depth of the tree\n",
        "* Minimum number of samples required to split a node\n",
        "* Minimum information gain or Gini reduction\n",
        "* Maximum number of leaf nodes\n",
        "\n",
        "### Why Pre-Pruning Is Used:\n",
        "\n",
        "* Prevents the model from becoming too complex\n",
        "* Reduces overfitting\n",
        "* Improves generalization on unseen data\n",
        "* Saves computation time\n",
        "\n",
        "**In simple words:**\n",
        "Pre-pruning stops the decision tree from growing too deep, helping it remain simpler and more accurate on new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "f2XmM4LA2mOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 : Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Answer. Here is a simple Python program that trains a Decision Tree Classifier using Gini Impurity and prints the feature importances."
      ],
      "metadata": {
        "id": "zF0mHQYi230G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create feature names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(feature_names, model.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "ht4Itat93HKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- criterion='gini' specifies the use of Gini Impurity\n",
        "\n",
        "- fit() trains the decision tree model\n",
        "\n",
        "- feature_importances_ shows how important each feature is in making decisions"
      ],
      "metadata": {
        "id": "zkxa-nlM3Jpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 : What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer. A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** tasks.\n",
        "\n",
        "SVM works by finding a **decision boundary (called a hyperplane)** that best separates data points of different classes. The goal is to choose the hyperplane that **maximizes the margin**, which is the distance between the hyperplane and the nearest data points from each class. These closest data points are called **support vectors**.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "* SVM focuses on **maximizing the margin** between classes\n",
        "* Support vectors determine the position of the decision boundary\n",
        "* Can handle **linear and non-linear data** using kernel functions (like linear, polynomial, RBF)\n",
        "* Effective in **high-dimensional spaces**\n",
        "\n",
        "**In simple words:**\n",
        "SVM finds the best boundary that separates data into different classes with maximum safety margin.\n"
      ],
      "metadata": {
        "id": "Ga8DaCt63XCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 : What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer. The **Kernel Trick** in **Support Vector Machines (SVM)** is a technique that allows SVMs to **solve non-linear classification problems**.\n",
        "\n",
        "Normally, SVM works well when data is **linearly separable**. When the data is non-linear, the kernel trick **transforms the data into a higher-dimensional space** where a linear separation becomes possible‚Äî**without explicitly computing that transformation**.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "* Kernels compute the similarity between data points\n",
        "* Avoids heavy computation of mapping to high dimensions\n",
        "* Makes SVM efficient for complex, non-linear patterns\n",
        "\n",
        "### Common Kernel Functions:\n",
        "\n",
        "* **Linear Kernel** ‚Äì for linearly separable data\n",
        "* **Polynomial Kernel** ‚Äì for curved boundaries\n",
        "* **RBF (Gaussian) Kernel** ‚Äì most commonly used for non-linear data\n",
        "* **Sigmoid Kernel** ‚Äì similar to neural networks\n",
        "\n",
        "**In simple words:**\n",
        "The kernel trick helps SVM draw **non-linear decision boundaries** by working in a higher-dimensional space without actually going there.\n"
      ],
      "metadata": {
        "id": "AZu1kvd43n0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 : Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Answer. Here is a simple Python program that trains two SVM classifiers (Linear and RBF kernels) on the Wine dataset and compares their accuracies.\n"
      ],
      "metadata": {
        "id": "zqsdsStX34B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "id": "fLPG0fgL4LVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- SVC(kernel='linear') trains an SVM with a Linear kernel\n",
        "\n",
        "- SVC(kernel='rbf') trains an SVM with an RBF kernel\n",
        "\n",
        "- accuracy_score() is used to compare model performance"
      ],
      "metadata": {
        "id": "dBhNwfOb4OHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 : What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Answer. The **Na√Øve Bayes classifier** is a **supervised machine learning algorithm** based on **Bayes‚Äô Theorem**, used mainly for **classification tasks** such as spam detection and text classification.\n",
        "\n",
        "It calculates the probability of a class given the input features and predicts the class with the **highest probability**.\n",
        "\n",
        "It is called **‚ÄúNa√Øve‚Äù** because it makes a **strong assumption** that **all features are independent of each other**, given the class. In real-world data, this assumption is usually not true, but the classifier still works surprisingly well in many cases.\n",
        "\n",
        "**In simple words:**\n",
        "Na√Øve Bayes predicts a class using probability and is called ‚Äúna√Øve‚Äù because it assumes all features work independently.\n"
      ],
      "metadata": {
        "id": "zJcb6lwU4Wpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes\n",
        "\n",
        "Answer. Here‚Äôs a clear explanation of the differences between the three main types of Na√Øve Bayes classifiers:\n",
        "\n",
        "**1. Gaussian Na√Øve Bayes**\n",
        "\n",
        "Assumes that continuous features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Probability is calculated using the mean and variance of the features for each class.\n",
        "\n",
        "Use case: Predicting numeric data, e.g., predicting flower species using petal lengths.\n",
        "\n",
        "Formula for feature probability:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùë¶\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ùúã\n",
        "ùúé\n",
        "ùë¶\n",
        "2\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùúá\n",
        "ùë¶\n",
        ")\n",
        "2\n",
        "2\n",
        "ùúé\n",
        "ùë¶\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£y)=\n",
        "2œÄœÉ\n",
        "y\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "exp(‚àí\n",
        "2œÉ\n",
        "y\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àíŒº\n",
        "y\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "**2. Multinomial Na√Øve Bayes**\n",
        "\n",
        "Designed for discrete count data (non-negative integers).\n",
        "\n",
        "Commonly used in text classification, where features are word counts or frequencies.\n",
        "\n",
        "Calculates the probability of each feature occurring multiple times in a class.\n",
        "\n",
        "Use case: Spam email detection, document classification.\n",
        "\n",
        "**3. Bernoulli Na√Øve Bayes**\n",
        "\n",
        "- Designed for binary/Boolean features (0 or 1).\n",
        "\n",
        "- Each feature is treated as present or absent.\n",
        "\n",
        "- Useful when you care about presence/absence of a feature, not frequency.\n",
        "\n",
        "**Use case:** Document classification using word presence/absence (binary features)."
      ],
      "metadata": {
        "id": "2A-0c9MA4ljY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10 : Question 10: Breast Cancer Dataset.\n",
        "\n",
        "Answer. Here‚Äôs a complete Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate its accuracy:"
      ],
      "metadata": {
        "id": "jgtISdRW5gAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Na√Øve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Gaussian Na√Øve Bayes Accuracy:\", accuracy)\n",
        "\n",
        "# Optional: Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "CjXrTZFZ56TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚úÖ Explanation:**\n",
        "\n",
        "1. GaussianNB() is used because the features are continuous numeric values.\n",
        "\n",
        "2. train_test_split() splits the dataset into training and testing sets.\n",
        "\n",
        "3. fit() trains the Gaussian Na√Øve Bayes model on the training data.\n",
        "\n",
        "4. predict() generates predictions on the test data.\n",
        "\n",
        "5. accuracy_score() calculates the model‚Äôs accuracy.\n",
        "\n",
        "**Sample Output (may vary slightly):**"
      ],
      "metadata": {
        "id": "S-ORlErR58wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Na√Øve Bayes Accuracy: 0.9415\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.94      0.96      0.95        64\n",
        "           1       0.94      0.92      0.93        53\n",
        "\n",
        "    accuracy                           0.94       117\n",
        "   macro avg       0.94      0.94      0.94       117\n",
        "weighted avg       0.94      0.94      0.94       117\n"
      ],
      "metadata": {
        "id": "i2aeKaYp6Ts8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In short:** The Gaussian Na√Øve Bayes classifier performs very well on the Breast Cancer dataset, achieving ~94% accuracy."
      ],
      "metadata": {
        "id": "AUjJxumR6VOs"
      }
    }
  ]
}